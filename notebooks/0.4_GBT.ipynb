{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.model_selection import train_test_split, cross_val_score, cross_val_predict\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, roc_curve, confusion_matrix\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "from lightgbm import LGBMClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from catboost import CatBoostClassifier\n",
    "from sklearn.ensemble import HistGradientBoostingClassifier\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, StratifiedKFold\n",
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = pd.read_csv(\"../outputs/imputed_X_train_SMOTE.csv\")\n",
    "y_train = pd.read_csv(\"../outputs/imputed_y_train_SMOTE.csv\")\n",
    "X_test = pd.read_csv(\"../outputs/imputed_X_test_scaled.csv\")\n",
    "y_test = pd.read_csv(\"../outputs/imputed_y_test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((981, 39), (330, 39))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "numerical_col = [\"Age\"]\n",
    "categorical_col = list(X_train.columns)\n",
    "categorical_col.remove(\"Age\")\n",
    "X_train[categorical_col] = X_train[categorical_col].astype('category')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['GenderCategory', 'RaceCategory', 'EthnicityCategory', 'Region',\n",
       "       'Glipizide_Final', 'Glimepiride_Final', 'Glyburide_Final',\n",
       "       'Metformin_Final', 'Pioglitazone_Final', 'Rosiglitazone_Final',\n",
       "       'Beta_Blockers_Final', 'ACE_Inhibitors_Final', 'ARB_Final',\n",
       "       'Diuretics_Final', 'PPI_Final', 'Levothyroxine_Final', 'CCB_Final',\n",
       "       'Vasodilators_Final', 'Statins_Final', 'Anti_Platelets_Final',\n",
       "       'Anti_Coagulants_Final', 'Steroids_Final', 'Heart_Disease_Final',\n",
       "       'Hypothyroid_Final', 'Anemia_Final', 'Kidney_Disease_Final',\n",
       "       'GERD_Final', 'Neuropathy_Final', 'Eye_Disorder_Final',\n",
       "       'Atherosclerosis_Final', 'Alzheimer_Final', 'FootUlcer_Final',\n",
       "       'Abnormal_Glucose_Final', 'DMScreen_Final', 'A1C_Final',\n",
       "       'GlucoseTest_Final', 'DM_Drugs', 'AnyLab'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "categorical_features = X_train.columns[X_train.dtypes == 'category']\n",
    "cat_col_numbers = [X_train.columns.get_loc(col) for col in X_train.select_dtypes(include=\"category\")]\n",
    "categorical_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "lbl = LabelEncoder()\n",
    "for col in categorical_col:\n",
    "    X_train[col] = lbl.fit_transform(X_train[col])\n",
    "    X_test[col] = lbl.transform(X_test[col])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Int64Index([4, 5, 18, 24, 25, 28, 32, 34, 36], dtype='int64')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = LogisticRegression(penalty=\"l1\", solver = \"liblinear\")\n",
    "clf.fit(X_train,y_train.values.ravel())\n",
    "clf_coeff = pd.DataFrame(clf.coef_, columns=X_train.columns)\n",
    "clf_coeff = clf_coeff.T.reset_index()\n",
    "clf_coeff[clf_coeff[0] == 0].index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_splits = 5\n",
    "\n",
    "model_params = {\n",
    "    \"cat_1\" : {\n",
    "        \"objective\" : \"CrossEntropy\",\n",
    "        \"eval_metric\" : 'AUC',\n",
    "        \"learning_rate\" : 0.001, \n",
    "        \"max_depth\" : 7,\n",
    "        # \"random_state\" : 56,\n",
    "        \"subsample\" : 0.35,\n",
    "        # 'early_stopping_rounds': 500,\n",
    "        # 'ignored_features' : [4, 5, 18, 24, 25, 28, 32, 34, 36],\n",
    "        \"n_estimators\": 10,\n",
    "        \"cat_features\" : cat_col_numbers,\n",
    "        \"verbose\" : 0        \n",
    "    },\n",
    "\n",
    "\n",
    "    \"hgb_1\" : {\n",
    "        \"learning_rate\":0.015,\n",
    "        \"n_iter_no_change\":100,\n",
    "        \"l2_regularization\" : 0.02,\n",
    "        \"random_state\" : 42,\n",
    "        \"categorical_features\" : cat_col_numbers\n",
    "        \n",
    "    },\n",
    "\n",
    "    \"lgbm_1\" :\n",
    "    {\n",
    "        \"objective\" : \"binary\",\n",
    "        \"metric\" : 'AUC',\n",
    "        \"learning_rate\" : 0.018, \n",
    "        \"max_depth\" : 15,\n",
    "        \"random_state\" : 56,\n",
    "        \"reg_alpha\" : 0.01246,\n",
    "        \"reg_lambda\": 0.023483,\n",
    "        \"subsample\" : 0.35,\n",
    "        \"colsample_bytree\" : 0.8,\n",
    "        \"verbose\" : 0,\n",
    "        \"num_leaves\" : 16,\n",
    "        \"categorical_features\" : cat_col_numbers,\n",
    "        \"device\" : \"gpu\",\n",
    "        \"verbose\" : -1,\n",
    "    }\n",
    "}\n",
    "\n",
    "hgb_1 = HistGradientBoostingClassifier(**model_params[\"hgb_1\"])\n",
    "lgbm_1 = LGBMClassifier(**model_params[\"lgbm_1\"])\n",
    "cat_1 = CatBoostClassifier(**model_params[\"cat_1\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "oofs_dict = {model + \"_oof\" : np.zeros(len(X_train)) for model in model_params.keys()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval(y_true, y_pred, y_pred_proba):\n",
    "    acc_score = accuracy_score(y_true, y_pred)\n",
    "    conf_mat = confusion_matrix(y_true,y_pred)\n",
    "\n",
    "    prec_score = precision_score(y_true, y_pred) \n",
    "    rec_score = recall_score(y_true, y_pred)\n",
    "    F1_score = f1_score(y_true, y_pred)\n",
    "\n",
    "    roc_auc = roc_auc_score(y_true, y_pred)\n",
    "    roc_auc_predict_proba = roc_auc_score(y_true, y_pred_proba)\n",
    "\n",
    "    print(f'Accuracy: {acc_score}')\n",
    "\n",
    "    print(\"Precision_score :\", prec_score)\n",
    "    print(\"recall_score :\", rec_score)\n",
    "    print(\"f1_score:\", F1_score)\n",
    "\n",
    "    print(f'ROC AUC score: {roc_auc}')\n",
    "    print(f'ROC AUC PROBA: {roc_auc_predict_proba}')\n",
    "\n",
    "    print(conf_mat)\n",
    "\n",
    "    print(\"==\" * 25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================== Fold :  0 ====================\n",
      "Accuracy: 0.8121827411167513\n",
      "Precision_score : 0.7857142857142857\n",
      "recall_score : 0.908256880733945\n",
      "f1_score: 0.8425531914893617\n",
      "ROC AUC score: 0.8007193494578816\n",
      "ROC AUC PROBA: 0.855660967472894\n",
      "[[61 27]\n",
      " [10 99]]\n",
      "==================================================\n",
      "Accuracy: 0.817258883248731\n",
      "Precision_score : 0.7829457364341085\n",
      "recall_score : 0.926605504587156\n",
      "f1_score: 0.8487394957983192\n",
      "ROC AUC score: 0.8042118432026689\n",
      "ROC AUC PROBA: 0.8876146788990826\n",
      "[[ 60  28]\n",
      " [  8 101]]\n",
      "==================================================\n",
      "Accuracy: 0.8477157360406091\n",
      "Precision_score : 0.8062015503875969\n",
      "recall_score : 0.9541284403669725\n",
      "f1_score: 0.8739495798319327\n",
      "ROC AUC score: 0.8350187656380317\n",
      "ROC AUC PROBA: 0.8990825688073395\n",
      "[[ 63  25]\n",
      " [  5 104]]\n",
      "==================================================\n",
      "==================== Fold :  1 ====================\n",
      "Accuracy: 0.7755102040816326\n",
      "Precision_score : 0.751937984496124\n",
      "recall_score : 0.8899082568807339\n",
      "f1_score: 0.8151260504201681\n",
      "ROC AUC score: 0.7610460824633556\n",
      "ROC AUC PROBA: 0.8795739744806496\n",
      "[[55 32]\n",
      " [12 97]]\n",
      "==================================================\n",
      "Accuracy: 0.8316326530612245\n",
      "Precision_score : 0.8114754098360656\n",
      "recall_score : 0.908256880733945\n",
      "f1_score: 0.8571428571428572\n",
      "ROC AUC score: 0.8219445323209955\n",
      "ROC AUC PROBA: 0.890066434672572\n",
      "[[64 23]\n",
      " [10 99]]\n",
      "==================================================\n",
      "Accuracy: 0.8112244897959183\n",
      "Precision_score : 0.7686567164179104\n",
      "recall_score : 0.944954128440367\n",
      "f1_score: 0.8477366255144032\n",
      "ROC AUC score: 0.7943161446799536\n",
      "ROC AUC PROBA: 0.8875883159337763\n",
      "[[ 56  31]\n",
      " [  6 103]]\n",
      "==================================================\n",
      "==================== Fold :  2 ====================\n",
      "Accuracy: 0.8469387755102041\n",
      "Precision_score : 0.8319327731092437\n",
      "recall_score : 0.908256880733945\n",
      "f1_score: 0.868421052631579\n",
      "ROC AUC score: 0.8391859116313404\n",
      "ROC AUC PROBA: 0.8949172202889382\n",
      "[[67 20]\n",
      " [10 99]]\n",
      "==================================================\n",
      "Accuracy: 0.8826530612244898\n",
      "Precision_score : 0.8524590163934426\n",
      "recall_score : 0.9541284403669725\n",
      "f1_score: 0.9004329004329004\n",
      "ROC AUC score: 0.8736159443214173\n",
      "ROC AUC PROBA: 0.9295581567014659\n",
      "[[ 69  18]\n",
      " [  5 104]]\n",
      "==================================================\n",
      "Accuracy: 0.8469387755102041\n",
      "Precision_score : 0.8211382113821138\n",
      "recall_score : 0.926605504587156\n",
      "f1_score: 0.8706896551724138\n",
      "ROC AUC score: 0.8368659706843826\n",
      "ROC AUC PROBA: 0.9274491194769587\n",
      "[[ 65  22]\n",
      " [  8 101]]\n",
      "==================================================\n",
      "==================== Fold :  3 ====================\n",
      "Accuracy: 0.8163265306122449\n",
      "Precision_score : 0.7967479674796748\n",
      "recall_score : 0.8990825688073395\n",
      "f1_score: 0.8448275862068967\n",
      "ROC AUC score: 0.8058631234841295\n",
      "ROC AUC PROBA: 0.8897500790888959\n",
      "[[62 25]\n",
      " [11 98]]\n",
      "==================================================\n",
      "Accuracy: 0.8622448979591837\n",
      "Precision_score : 0.8416666666666667\n",
      "recall_score : 0.926605504587156\n",
      "f1_score: 0.8820960698689956\n",
      "ROC AUC score: 0.8541073499947274\n",
      "ROC AUC PROBA: 0.9057260360645366\n",
      "[[ 68  19]\n",
      " [  8 101]]\n",
      "==================================================\n",
      "Accuracy: 0.8520408163265306\n",
      "Precision_score : 0.8278688524590164\n",
      "recall_score : 0.926605504587156\n",
      "f1_score: 0.8744588744588746\n",
      "ROC AUC score: 0.8426130971211641\n",
      "ROC AUC PROBA: 0.9091532215543604\n",
      "[[ 66  21]\n",
      " [  8 101]]\n",
      "==================================================\n",
      "==================== Fold :  4 ====================\n",
      "Accuracy: 0.8112244897959183\n",
      "Precision_score : 0.7769230769230769\n",
      "recall_score : 0.926605504587156\n",
      "f1_score: 0.8451882845188285\n",
      "ROC AUC score: 0.7966360856269113\n",
      "ROC AUC PROBA: 0.867974269745861\n",
      "[[ 58  29]\n",
      " [  8 101]]\n",
      "==================================================\n",
      "Accuracy: 0.8316326530612245\n",
      "Precision_score : 0.796875\n",
      "recall_score : 0.9357798165137615\n",
      "f1_score: 0.8607594936708861\n",
      "ROC AUC score: 0.8184646209005588\n",
      "ROC AUC PROBA: 0.8972898871665085\n",
      "[[ 61  26]\n",
      " [  7 102]]\n",
      "==================================================\n",
      "Accuracy: 0.826530612244898\n",
      "Precision_score : 0.7819548872180451\n",
      "recall_score : 0.9541284403669725\n",
      "f1_score: 0.859504132231405\n",
      "ROC AUC score: 0.8103975535168196\n",
      "ROC AUC PROBA: 0.8910155014236001\n",
      "[[ 58  29]\n",
      " [  5 104]]\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "kf = StratifiedKFold(n_splits=5, random_state=42, shuffle=True)\n",
    "model_auc = {model + \"_auc\" : []  for model in model_params.keys()}\n",
    "\n",
    "\n",
    "for num_fold, (train_index, val_index) in enumerate(kf.split(X_train,y_train.values)):\n",
    "    \n",
    "    print(\"=\"*20, \"Fold : \", num_fold, \"=\"*20)\n",
    "    X_train_fold = X_train.iloc[train_index]\n",
    "    X_val_fold = X_train.iloc[val_index]\n",
    "    y_train_fold = y_train.iloc[train_index].values.ravel()\n",
    "    y_val_fold = y_train.iloc[val_index].values.ravel()\n",
    "    \n",
    "\n",
    "    cat_1.fit(X_train_fold,y_train_fold, eval_set=(X_val_fold,y_val_fold))\n",
    "    oofs_dict[\"cat_1_oof\"][val_index] = cat_1.predict(X_val_fold)\n",
    "    y_pred = cat_1.predict(X_val_fold)\n",
    "    y_pred_proba = cat_1.predict_proba(X_val_fold)[:, 1]\n",
    "    # print('Fold', num_fold, 'CAT_1: ')\n",
    "    model_auc[\"cat_1_auc\"].append(roc_auc_score(y_val_fold, y_pred))\n",
    "    eval(y_val_fold,y_pred, y_pred_proba)\n",
    "\n",
    "\n",
    "    hgb_1.fit(X_train_fold, y_train_fold)\n",
    "    oofs_dict[\"hgb_1_oof\"][val_index] = hgb_1.predict(X_val_fold)\n",
    "    y_pred = hgb_1.predict(X_val_fold)\n",
    "    y_pred_proba = hgb_1.predict_proba(X_val_fold)[:, 1]\n",
    "    model_auc[\"hgb_1_auc\"].append(roc_auc_score(y_val_fold, y_pred))\n",
    "    # print('Fold', num_fold, 'HGB_1: ')\n",
    "    eval(y_val_fold,y_pred, y_pred_proba)\n",
    "    \n",
    "    \n",
    "    lgbm_1.fit(X_train_fold, y_train_fold)\n",
    "    oofs_dict[\"lgbm_1_oof\"][val_index] = lgbm_1.predict(X_val_fold)\n",
    "    y_pred = lgbm_1.predict(X_val_fold)\n",
    "    y_pred_proba = lgbm_1.predict_proba(X_val_fold)[:, 1]\n",
    "    # print('Fold', num_fold, 'LGBM_1: ')\n",
    "    model_auc[\"lgbm_1_auc\"].append(roc_auc_score(y_val_fold, y_pred))\n",
    "    eval(y_val_fold,y_pred, y_pred_proba)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.8006901105327238, 0.8344688581480737, 0.8238423063280702)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(model_auc[\"cat_1_auc\"])/5, sum(model_auc[\"hgb_1_auc\"])/5, sum(model_auc[\"lgbm_1_auc\"])/5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HGB is better classifier than LGBM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training HGB on full train data and testing it before Hyperparameter tuning:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 0 HGB_1: \n",
      "ROC:  0.8326209341117599\n",
      "Fold 1 HGB_1: \n",
      "ROC:  0.8690287883581146\n",
      "Fold 2 HGB_1: \n",
      "ROC:  0.7943688706105663\n",
      "Fold 3 HGB_1: \n",
      "ROC:  0.8047031530106507\n",
      "Fold 4 HGB_1: \n",
      "ROC:  0.7886217441737846\n",
      "Average ROC:  0.8178686980529752\n"
     ]
    }
   ],
   "source": [
    "roc_list = []\n",
    "kf = StratifiedKFold(n_splits=5, random_state=2023, shuffle=True)\n",
    "for num_fold, (train_index, val_index) in enumerate(kf.split(X_train,y_train.values)):\n",
    "\n",
    "    \n",
    "    # print(\"=\"*20, \"Fold : \", num_fold, \"=\"*20)\n",
    "    X_train_fold = X_train.iloc[train_index]\n",
    "    X_val_fold = X_train.iloc[val_index]\n",
    "    y_train_fold = y_train.iloc[train_index].values.ravel()\n",
    "    y_val_fold = y_train.iloc[val_index].values.ravel()\n",
    "\n",
    "    \n",
    "    hgb_1.fit(X_train_fold, y_train_fold)\n",
    "    oofs_dict[\"hgb_1_oof\"][val_index] = hgb_1.predict(X_val_fold)\n",
    "    y_pred = hgb_1.predict(X_val_fold)\n",
    "    y_pred_proba = hgb_1.predict_proba(X_val_fold)[:, 1]\n",
    "    print('Fold', num_fold, 'HGB_1: ')\n",
    "    roc = roc_auc_score(y_val_fold,y_pred)\n",
    "    print(\"ROC: \", roc)\n",
    "    roc_list.append(roc)\n",
    "\n",
    "print(\"Average ROC: \",sum(roc_list)/len(roc_list))\n",
    "    # eval(y_val_fold,y_pred, y_pred_proba)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna\n",
    "\n",
    "def objective(trial,data=X_train,target=y_train.values.ravel()):\n",
    "    train_x, val_x, train_y, val_y = train_test_split(data, target, test_size=0.33,random_state=42, stratify=target)\n",
    "    params = {\n",
    "        'l2_regularization': trial.suggest_float('l2_regularization',1e-2,10, log = True),\n",
    "        'early_stopping': trial.suggest_categorical('early_stopping', [False, True]),\n",
    "        'learning_rate': trial.suggest_float('learning_rate', 0.001,0.1, log = True),\n",
    "        # 'max_iter': trial.suggest_categorical('max_iter', [750, 1000, 1250, 1500]),\n",
    "        'max_depth': trial.suggest_int('max_depth', 2,8),\n",
    "        'max_bins': trial.suggest_int('max_bins', 5,255),\n",
    "        'min_samples_leaf': trial.suggest_int('min_samples_leaf', 3,8),\n",
    "        'max_leaf_nodes': trial.suggest_int('max_leaf_nodes', 20,80),\n",
    "        \"categorical_features\" : cat_col_numbers\n",
    "    }\n",
    "\n",
    "    model = HistGradientBoostingClassifier(**params)\n",
    "    model.fit(train_x, train_y)\n",
    "    predictions = model.predict(val_x)\n",
    "    auc = roc_auc_score(val_y, predictions)\n",
    "    print(auc)\n",
    "    return auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8113636363636365\n",
      "0.8\n",
      "0.8454545454545455\n",
      "0.8113636363636365\n",
      "0.7886363636363636\n",
      "0.7931818181818182\n",
      "0.8750000000000001\n",
      "0.7886363636363636\n",
      "0.7886363636363636\n",
      "0.7681818181818181\n",
      "0.8636363636363635\n",
      "0.85\n",
      "0.8750000000000001\n",
      "0.8727272727272727\n",
      "0.85\n",
      "0.8659090909090909\n",
      "0.8318181818181818\n",
      "0.8704545454545454\n",
      "0.8909090909090909\n",
      "0.7909090909090909\n",
      "0.8727272727272727\n",
      "0.8750000000000001\n",
      "0.8363636363636363\n",
      "0.8840909090909091\n",
      "0.7954545454545455\n",
      "0.825\n",
      "0.825\n",
      "0.8704545454545454\n",
      "0.8272727272727273\n",
      "0.8272727272727273\n",
      "0.8159090909090909\n",
      "0.8954545454545454\n",
      "0.9159090909090909\n",
      "0.8659090909090909\n",
      "0.8568181818181818\n",
      "0.825\n",
      "0.85\n",
      "0.8659090909090909\n",
      "0.8704545454545454\n",
      "0.8045454545454546\n",
      "0.825\n",
      "0.8727272727272727\n",
      "0.8545454545454545\n",
      "0.859090909090909\n",
      "0.8840909090909091\n",
      "0.8159090909090909\n",
      "0.8931818181818181\n",
      "0.8409090909090909\n",
      "0.8386363636363636\n",
      "0.8840909090909091\n",
      "0.9159090909090909\n",
      "0.8954545454545454\n",
      "0.9068181818181817\n",
      "0.8954545454545454\n",
      "0.8977272727272727\n",
      "0.9068181818181817\n",
      "0.8977272727272727\n",
      "0.8977272727272727\n",
      "0.8931818181818181\n",
      "0.9295454545454547\n",
      "0.9272727272727272\n",
      "0.8954545454545454\n",
      "0.9181818181818181\n",
      "0.8840909090909091\n",
      "0.8863636363636365\n",
      "0.8863636363636365\n",
      "0.9272727272727272\n",
      "0.8636363636363635\n",
      "0.8954545454545454\n",
      "0.8863636363636365\n",
      "0.9068181818181817\n",
      "0.9181818181818181\n",
      "0.9181818181818181\n",
      "0.8863636363636365\n",
      "0.8931818181818181\n",
      "0.8954545454545454\n",
      "0.9068181818181817\n",
      "0.9045454545454545\n",
      "0.9045454545454545\n",
      "0.8863636363636365\n",
      "0.9272727272727272\n",
      "0.9272727272727272\n",
      "0.9204545454545455\n",
      "0.9090909090909091\n",
      "0.925\n",
      "0.9272727272727272\n",
      "0.9386363636363637\n",
      "0.9068181818181817\n",
      "0.8977272727272727\n",
      "0.9477272727272728\n",
      "0.9136363636363637\n",
      "0.9159090909090909\n",
      "0.925\n",
      "0.8954545454545454\n",
      "0.9159090909090909\n",
      "0.9295454545454547\n",
      "0.9068181818181817\n",
      "0.9181818181818181\n",
      "0.9454545454545454\n",
      "0.9045454545454545\n",
      "0.8727272727272727\n",
      "0.9159090909090909\n",
      "0.9159090909090909\n",
      "0.9272727272727272\n",
      "0.9363636363636363\n",
      "0.9386363636363637\n",
      "0.9045454545454545\n",
      "0.9181818181818181\n",
      "0.9295454545454547\n",
      "0.9068181818181817\n",
      "0.9181818181818181\n",
      "0.9272727272727272\n",
      "0.9181818181818181\n",
      "0.9386363636363637\n",
      "0.8272727272727273\n",
      "0.9159090909090909\n",
      "0.9159090909090909\n",
      "0.9045454545454545\n",
      "0.9363636363636363\n",
      "0.925\n",
      "0.9159090909090909\n",
      "0.9159090909090909\n",
      "0.9068181818181817\n",
      "0.9386363636363637\n",
      "0.9295454545454547\n",
      "0.9159090909090909\n",
      "0.9363636363636363\n",
      "0.9272727272727272\n",
      "0.9159090909090909\n",
      "0.9272727272727272\n",
      "0.9068181818181817\n",
      "0.9159090909090909\n",
      "0.9272727272727272\n",
      "0.9386363636363637\n",
      "0.9159090909090909\n",
      "0.9068181818181817\n",
      "0.9363636363636363\n",
      "0.925\n",
      "0.9045454545454545\n",
      "0.9159090909090909\n",
      "0.8931818181818181\n",
      "0.9181818181818181\n",
      "0.9068181818181817\n",
      "0.9159090909090909\n",
      "0.9181818181818181\n",
      "0.9022727272727273\n",
      "0.9272727272727272\n",
      "0.9181818181818181\n",
      "0.8159090909090909\n",
      "0.9477272727272728\n",
      "0.9386363636363637\n",
      "0.9181818181818181\n",
      "0.9363636363636363\n",
      "0.9159090909090909\n",
      "0.9068181818181817\n",
      "0.9386363636363637\n",
      "0.9136363636363637\n",
      "0.9181818181818181\n",
      "0.9022727272727273\n",
      "0.9159090909090909\n",
      "0.8363636363636363\n",
      "0.9363636363636363\n",
      "0.9159090909090909\n",
      "0.9159090909090909\n",
      "0.9272727272727272\n",
      "0.9295454545454547\n",
      "0.925\n",
      "0.925\n",
      "0.9295454545454547\n",
      "0.9045454545454545\n",
      "0.9159090909090909\n",
      "0.9181818181818181\n",
      "0.9159090909090909\n",
      "0.95\n",
      "0.9295454545454547\n",
      "0.9068181818181817\n",
      "0.7681818181818181\n",
      "0.9386363636363637\n",
      "0.9159090909090909\n",
      "0.9113636363636364\n",
      "0.9045454545454545\n",
      "0.9159090909090909\n",
      "0.9295454545454547\n",
      "0.9363636363636363\n",
      "0.8909090909090909\n",
      "0.9477272727272728\n",
      "0.8886363636363636\n",
      "0.925\n",
      "0.9136363636363637\n",
      "0.9068181818181817\n",
      "0.9363636363636363\n",
      "0.9477272727272728\n",
      "0.9090909090909091\n",
      "0.9272727272727272\n",
      "0.9181818181818181\n",
      "0.925\n",
      "0.9272727272727272\n",
      "0.9068181818181817\n",
      "0.8795454545454545\n",
      "0.925\n",
      "0.9363636363636363\n",
      "0.9363636363636363\n",
      "0.925\n",
      "0.925\n",
      "0.9181818181818181\n",
      "0.9272727272727272\n",
      "0.9045454545454545\n",
      "0.9363636363636363\n",
      "0.9272727272727272\n",
      "0.8954545454545454\n",
      "0.8386363636363636\n",
      "0.9068181818181817\n",
      "0.9340909090909091\n",
      "0.9136363636363637\n",
      "0.9363636363636363\n",
      "0.9272727272727272\n",
      "0.9045454545454545\n",
      "0.9090909090909091\n",
      "0.9068181818181817\n",
      "0.9159090909090909\n",
      "0.9159090909090909\n",
      "0.9159090909090909\n",
      "0.9386363636363637\n",
      "0.9068181818181817\n",
      "0.9477272727272728\n",
      "0.7636363636363637\n",
      "0.9477272727272728\n",
      "0.9136363636363637\n",
      "0.9136363636363637\n",
      "0.9272727272727272\n",
      "0.925\n",
      "0.9181818181818181\n",
      "0.9045454545454545\n",
      "0.9204545454545455\n",
      "0.9477272727272728\n",
      "0.9272727272727272\n",
      "0.9136363636363637\n",
      "0.9181818181818181\n",
      "0.9272727272727272\n",
      "0.9477272727272728\n",
      "0.5\n",
      "0.8954545454545454\n",
      "0.9477272727272728\n",
      "0.8909090909090909\n",
      "0.925\n",
      "0.9272727272727272\n",
      "0.925\n",
      "0.9363636363636363\n",
      "0.8863636363636365\n",
      "0.9045454545454545\n",
      "0.9159090909090909\n",
      "0.8022727272727274\n",
      "0.8931818181818181\n",
      "0.9272727272727272\n",
      "0.7795454545454545\n",
      "0.9068181818181817\n",
      "0.925\n",
      "0.8704545454545454\n",
      "0.8681818181818182\n",
      "0.9136363636363637\n",
      "0.8750000000000001\n",
      "0.9272727272727272\n",
      "0.9386363636363637\n",
      "0.8568181818181818\n",
      "0.9272727272727272\n",
      "0.925\n",
      "0.9022727272727273\n",
      "0.8840909090909091\n",
      "0.9068181818181817\n",
      "0.7681818181818181\n",
      "0.9272727272727272\n",
      "0.925\n",
      "0.9045454545454545\n",
      "0.9159090909090909\n",
      "0.9272727272727272\n",
      "0.9045454545454545\n",
      "0.9159090909090909\n",
      "0.8795454545454545\n",
      "0.9272727272727272\n",
      "0.925\n",
      "0.8568181818181818\n",
      "0.9272727272727272\n",
      "0.9159090909090909\n",
      "0.9159090909090909\n",
      "0.9363636363636363\n",
      "0.8363636363636363\n",
      "0.9136363636363637\n",
      "0.8113636363636365\n",
      "0.8954545454545454\n",
      "0.8750000000000001\n",
      "0.925\n",
      "0.925\n",
      "0.9272727272727272\n",
      "0.9159090909090909\n",
      "0.9386363636363637\n",
      "0.868181818181818\n",
      "0.925\n",
      "0.9181818181818181\n",
      "0.9068181818181817\n",
      "0.8227272727272729\n",
      "0.9022727272727273\n",
      "0.9181818181818181\n",
      "0.8340909090909091\n",
      "0.9045454545454545\n",
      "0.9363636363636363\n",
      "0.9113636363636364\n",
      "0.8\n",
      "0.9022727272727273\n",
      "0.9295454545454547\n",
      "0.9022727272727273\n",
      "0.9181818181818181\n",
      "0.9272727272727272\n",
      "0.9363636363636363\n",
      "0.9136363636363637\n",
      "0.9295454545454547\n",
      "0.8909090909090909\n",
      "0.9159090909090909\n",
      "0.8340909090909091\n",
      "0.9045454545454545\n",
      "0.9363636363636363\n",
      "0.8977272727272727\n",
      "0.9272727272727272\n",
      "0.8477272727272727\n",
      "0.8886363636363636\n",
      "0.925\n",
      "0.9068181818181817\n",
      "0.8795454545454545\n",
      "0.9045454545454545\n",
      "0.9159090909090909\n",
      "0.9022727272727273\n",
      "0.8386363636363636\n",
      "0.85\n",
      "0.9159090909090909\n",
      "0.925\n",
      "0.8977272727272727\n",
      "0.9272727272727272\n",
      "0.9227272727272727\n",
      "0.9068181818181817\n",
      "0.8795454545454545\n",
      "0.9386363636363637\n",
      "0.9090909090909091\n",
      "0.9295454545454547\n",
      "0.9159090909090909\n",
      "0.9068181818181817\n",
      "0.9159090909090909\n",
      "0.8568181818181818\n",
      "0.7909090909090909\n",
      "0.9272727272727272\n",
      "0.9272727272727272\n",
      "0.8477272727272727\n",
      "0.8795454545454545\n",
      "0.9272727272727272\n",
      "0.8750000000000001\n",
      "0.9045454545454545\n",
      "0.8840909090909091\n",
      "0.9272727272727272\n",
      "0.9136363636363637\n",
      "0.9022727272727273\n",
      "0.9159090909090909\n",
      "0.9363636363636363\n",
      "0.8795454545454545\n",
      "0.9068181818181817\n",
      "0.9181818181818181\n",
      "0.9272727272727272\n",
      "0.925\n",
      "0.8840909090909091\n",
      "0.9068181818181817\n",
      "0.8886363636363636\n",
      "0.9159090909090909\n",
      "0.8340909090909091\n",
      "0.7681818181818181\n",
      "0.8886363636363636\n",
      "0.9363636363636363\n",
      "0.9136363636363637\n",
      "0.9295454545454547\n",
      "0.8977272727272727\n",
      "0.8159090909090909\n",
      "0.8477272727272727\n",
      "0.7681818181818181\n",
      "0.9159090909090909\n",
      "0.9045454545454545\n",
      "0.9181818181818181\n",
      "0.925\n",
      "0.8818181818181816\n",
      "0.9272727272727272\n",
      "0.9477272727272728\n",
      "0.9136363636363637\n",
      "0.8022727272727274\n",
      "0.8954545454545454\n",
      "0.859090909090909\n",
      "0.9159090909090909\n",
      "0.9386363636363637\n",
      "0.9045454545454545\n",
      "0.9136363636363637\n",
      "0.9272727272727272\n",
      "0.8977272727272727\n",
      "0.8772727272727272\n",
      "0.9068181818181817\n",
      "0.8977272727272727\n",
      "0.9386363636363637\n",
      "0.9272727272727272\n",
      "0.9159090909090909\n",
      "0.9477272727272728\n",
      "0.8568181818181818\n",
      "0.9159090909090909\n",
      "0.8954545454545454\n",
      "0.9159090909090909\n",
      "0.8863636363636365\n",
      "0.9136363636363637\n",
      "0.8909090909090909\n",
      "0.8954545454545454\n",
      "0.7659090909090909\n",
      "0.7704545454545455\n",
      "0.9136363636363637\n",
      "0.9159090909090909\n",
      "0.9068181818181817\n",
      "0.9386363636363637\n",
      "0.8363636363636363\n",
      "0.9045454545454545\n",
      "0.925\n",
      "0.9159090909090909\n",
      "0.9272727272727272\n",
      "0.9295454545454547\n",
      "0.9136363636363637\n",
      "0.7636363636363637\n",
      "0.7545454545454544\n",
      "0.9022727272727273\n",
      "0.825\n",
      "0.8954545454545454\n",
      "0.9136363636363637\n",
      "0.7886363636363636\n",
      "0.9045454545454545\n",
      "0.9386363636363637\n",
      "0.8568181818181818\n",
      "0.9386363636363637\n",
      "0.7613636363636365\n",
      "0.9159090909090909\n",
      "0.8113636363636365\n",
      "0.9159090909090909\n",
      "0.8681818181818182\n",
      "0.9068181818181817\n",
      "0.9045454545454545\n",
      "0.9181818181818181\n",
      "0.9090909090909091\n",
      "0.9159090909090909\n",
      "0.9386363636363637\n",
      "0.9045454545454545\n",
      "0.7818181818181819\n",
      "0.9181818181818181\n",
      "0.925\n",
      "0.9136363636363637\n",
      "0.9386363636363637\n",
      "0.9159090909090909\n",
      "0.9159090909090909\n",
      "0.9068181818181817\n",
      "0.9022727272727273\n",
      "0.9022727272727273\n",
      "0.7818181818181819\n",
      "0.9045454545454545\n",
      "0.7681818181818181\n",
      "0.9068181818181817\n",
      "0.9477272727272728\n",
      "0.8136363636363637\n",
      "0.925\n",
      "0.8113636363636365\n",
      "0.9272727272727272\n",
      "0.9045454545454545\n",
      "0.9181818181818181\n",
      "0.8772727272727272\n",
      "0.7681818181818181\n",
      "0.8386363636363636\n",
      "0.925\n",
      "0.8681818181818182\n",
      "0.8568181818181818\n",
      "0.7636363636363637\n",
      "0.8999999999999999\n",
      "0.9181818181818181\n",
      "0.9136363636363637\n",
      "0.8340909090909091\n",
      "0.9204545454545455\n",
      "0.8954545454545454\n",
      "0.9477272727272728\n",
      "0.9136363636363637\n",
      "0.8159090909090909\n",
      "0.8954545454545454\n",
      "0.9272727272727272\n",
      "0.9159090909090909\n",
      "0.925\n",
      "0.8340909090909091\n",
      "0.5\n",
      "0.8954545454545454\n",
      "0.8954545454545454\n",
      "0.8454545454545455\n",
      "0.8954545454545454\n",
      "0.9272727272727272\n",
      "0.9477272727272728\n",
      "0.9295454545454547\n",
      "0.9386363636363637\n",
      "0.9272727272727272\n",
      "0.8772727272727272\n",
      "0.8\n",
      "0.9386363636363637\n",
      "0.9159090909090909\n",
      "0.925\n",
      "0.8340909090909091\n",
      "0.7659090909090909\n",
      "0.7772727272727272\n",
      "0.9227272727272727\n",
      "0.925\n",
      "0.8681818181818182\n",
      "0.9045454545454545\n",
      "0.9295454545454547\n",
      "0.9272727272727272\n",
      "0.9477272727272728\n",
      "0.9272727272727272\n",
      "0.8704545454545454\n",
      "0.9045454545454545\n",
      "0.8363636363636363\n",
      "0.7681818181818181\n",
      "0.8295454545454546\n",
      "0.9272727272727272\n",
      "0.9181818181818181\n",
      "0.9272727272727272\n",
      "0.925\n",
      "0.8659090909090909\n",
      "0.9159090909090909\n",
      "0.9159090909090909\n",
      "0.9477272727272728\n",
      "0.9045454545454545\n",
      "0.8909090909090909\n",
      "0.7681818181818181\n",
      "0.8954545454545454\n",
      "0.9068181818181817\n",
      "0.9181818181818181\n",
      "0.7681818181818181\n",
      "0.8863636363636365\n",
      "0.9159090909090909\n",
      "0.9477272727272728\n",
      "0.9068181818181817\n",
      "0.8113636363636365\n",
      "0.7659090909090909\n",
      "0.9068181818181817\n",
      "0.9113636363636364\n",
      "0.9181818181818181\n",
      "0.9363636363636363\n",
      "0.7681818181818181\n",
      "0.7681818181818181\n",
      "0.925\n",
      "0.8954545454545454\n",
      "0.8931818181818181\n",
      "0.7636363636363637\n",
      "0.9181818181818181\n",
      "0.925\n",
      "0.8340909090909091\n",
      "0.9272727272727272\n",
      "0.7772727272727272\n",
      "0.7795454545454545\n",
      "0.9363636363636363\n",
      "0.8954545454545454\n",
      "0.9045454545454545\n",
      "0.7681818181818181\n",
      "0.8954545454545454\n",
      "0.9386363636363637\n",
      "0.8954545454545454\n",
      "0.9272727272727272\n",
      "0.8681818181818182\n",
      "0.7636363636363637\n",
      "0.7727272727272727\n",
      "0.7772727272727272\n",
      "0.9159090909090909\n",
      "0.7818181818181819\n",
      "0.9386363636363637\n",
      "0.9159090909090909\n",
      "0.9068181818181817\n",
      "0.8340909090909091\n",
      "0.8772727272727272\n",
      "0.925\n",
      "0.8568181818181818\n",
      "0.9340909090909091\n",
      "0.9068181818181817\n",
      "0.7931818181818182\n",
      "0.9159090909090909\n",
      "0.9272727272727272\n",
      "0.9363636363636363\n",
      "0.7681818181818181\n",
      "0.8386363636363636\n",
      "0.925\n",
      "0.8977272727272727\n",
      "0.9295454545454547\n",
      "0.8931818181818181\n",
      "0.9045454545454545\n",
      "0.9181818181818181\n",
      "0.8363636363636363\n",
      "0.9159090909090909\n",
      "0.8363636363636363\n",
      "0.7681818181818181\n",
      "0.8477272727272727\n",
      "0.9295454545454547\n",
      "0.9068181818181817\n",
      "0.8340909090909091\n",
      "0.8750000000000001\n",
      "0.8931818181818181\n",
      "0.9272727272727272\n",
      "0.7681818181818181\n",
      "0.7795454545454545\n",
      "0.7795454545454545\n",
      "0.8318181818181818\n",
      "0.8681818181818182\n",
      "0.9022727272727273\n",
      "0.9159090909090909\n",
      "0.8909090909090909\n",
      "0.9272727272727272\n",
      "0.8954545454545454\n",
      "0.9045454545454545\n",
      "0.9136363636363637\n",
      "0.7636363636363637\n",
      "0.8386363636363636\n",
      "0.8\n",
      "0.7659090909090909\n",
      "0.825\n",
      "0.7931818181818182\n",
      "0.7636363636363637\n",
      "0.9272727272727272\n",
      "0.9068181818181817\n",
      "0.8795454545454545\n",
      "0.8568181818181818\n",
      "0.9068181818181817\n",
      "0.7681818181818181\n",
      "0.8863636363636365\n",
      "0.9363636363636363\n",
      "0.7545454545454544\n",
      "0.9272727272727272\n",
      "0.9272727272727272\n",
      "0.6818181818181819\n",
      "0.9045454545454545\n",
      "0.7795454545454545\n",
      "0.9159090909090909\n",
      "0.8931818181818181\n",
      "0.9477272727272728\n",
      "0.9136363636363637\n",
      "0.9045454545454545\n",
      "0.775\n",
      "0.9136363636363637\n",
      "0.8931818181818181\n",
      "0.7545454545454544\n",
      "0.7772727272727272\n",
      "0.7772727272727272\n",
      "0.9022727272727273\n",
      "0.9068181818181817\n",
      "0.9159090909090909\n",
      "0.9272727272727272\n",
      "0.8954545454545454\n",
      "0.7659090909090909\n",
      "0.8772727272727272\n",
      "0.8772727272727272\n",
      "0.9068181818181817\n",
      "0.9386363636363637\n",
      "0.7818181818181819\n",
      "0.7704545454545455\n",
      "0.75\n",
      "0.8363636363636363\n",
      "0.9295454545454547\n",
      "0.8340909090909091\n",
      "0.9136363636363637\n",
      "0.9159090909090909\n",
      "0.7909090909090909\n",
      "0.9068181818181817\n",
      "0.9022727272727273\n",
      "0.9068181818181817\n",
      "0.9181818181818181\n",
      "0.8136363636363637\n",
      "0.9363636363636363\n",
      "0.9272727272727272\n",
      "0.9227272727272727\n",
      "0.8954545454545454\n",
      "0.8681818181818182\n",
      "0.9409090909090909\n",
      "0.9068181818181817\n",
      "0.9272727272727272\n",
      "0.9159090909090909\n",
      "0.9113636363636364\n",
      "0.7568181818181816\n",
      "0.8954545454545454\n",
      "0.8954545454545454\n",
      "0.8727272727272727\n",
      "0.75\n",
      "0.7545454545454544\n",
      "0.9159090909090909\n",
      "0.7954545454545455\n",
      "0.8818181818181816\n",
      "0.8477272727272727\n",
      "0.8022727272727274\n",
      "0.8772727272727272\n",
      "0.9045454545454545\n",
      "0.9272727272727272\n",
      "0.7681818181818181\n",
      "0.8659090909090909\n",
      "0.759090909090909\n",
      "0.7659090909090909\n",
      "0.9159090909090909\n",
      "0.9272727272727272\n",
      "0.6954545454545454\n",
      "0.925\n",
      "0.9068181818181817\n",
      "0.9022727272727273\n",
      "0.925\n",
      "0.9068181818181817\n",
      "0.9068181818181817\n",
      "0.8886363636363636\n",
      "0.8363636363636363\n",
      "0.8090909090909091\n",
      "0.925\n",
      "0.9068181818181817\n",
      "0.8795454545454545\n",
      "0.9136363636363637\n",
      "0.7545454545454544\n",
      "0.9363636363636363\n",
      "0.8954545454545454\n",
      "0.7545454545454544\n",
      "0.8181818181818182\n",
      "0.9045454545454545\n",
      "0.7704545454545455\n",
      "0.7681818181818181\n",
      "0.8568181818181818\n",
      "0.9159090909090909\n",
      "0.9136363636363637\n",
      "0.9272727272727272\n",
      "0.9045454545454545\n",
      "0.7545454545454544\n",
      "0.925\n",
      "0.8613636363636363\n",
      "0.859090909090909\n",
      "0.7545454545454544\n",
      "0.8954545454545454\n",
      "0.8363636363636363\n",
      "0.7886363636363636\n",
      "0.8272727272727273\n",
      "0.9136363636363637\n",
      "0.9363636363636363\n",
      "0.9204545454545455\n",
      "0.9045454545454545\n",
      "0.7636363636363637\n",
      "0.9068181818181817\n",
      "0.8840909090909091\n",
      "0.9363636363636363\n",
      "0.8954545454545454\n",
      "0.7636363636363637\n",
      "0.9045454545454545\n",
      "0.925\n",
      "0.8954545454545454\n",
      "0.9363636363636363\n",
      "0.9068181818181817\n",
      "0.9363636363636363\n",
      "0.9363636363636363\n",
      "0.8840909090909091\n",
      "0.8909090909090909\n",
      "0.8909090909090909\n",
      "0.8181818181818182\n",
      "0.6727272727272728\n",
      "0.7681818181818181\n",
      "0.8136363636363637\n",
      "0.8477272727272727\n",
      "0.8136363636363637\n",
      "0.9363636363636363\n",
      "0.7772727272727272\n",
      "0.9159090909090909\n",
      "0.9272727272727272\n",
      "0.9159090909090909\n",
      "0.9045454545454545\n",
      "0.9045454545454545\n",
      "0.7704545454545455\n",
      "0.925\n",
      "0.7659090909090909\n",
      "0.7636363636363637\n",
      "0.7636363636363637\n",
      "0.8863636363636365\n",
      "0.7909090909090909\n",
      "0.7704545454545455\n",
      "0.9295454545454547\n",
      "0.8386363636363636\n",
      "0.95\n",
      "0.8363636363636363\n",
      "0.9159090909090909\n",
      "0.825\n",
      "0.8568181818181818\n",
      "0.8818181818181816\n",
      "0.8863636363636365\n",
      "0.8159090909090909\n",
      "0.9159090909090909\n",
      "0.8750000000000001\n",
      "0.8113636363636365\n",
      "0.9159090909090909\n",
      "0.7704545454545455\n",
      "0.8568181818181818\n",
      "0.8954545454545454\n",
      "0.9181818181818181\n",
      "0.859090909090909\n",
      "0.925\n",
      "0.9022727272727273\n",
      "0.8772727272727272\n",
      "0.9363636363636363\n",
      "0.9045454545454545\n",
      "0.925\n",
      "0.9181818181818181\n",
      "0.9477272727272728\n",
      "0.9159090909090909\n",
      "0.9136363636363637\n",
      "0.925\n",
      "0.9340909090909091\n",
      "0.7772727272727272\n",
      "0.9159090909090909\n",
      "0.7636363636363637\n",
      "0.859090909090909\n",
      "0.9045454545454545\n",
      "0.9272727272727272\n",
      "0.9136363636363637\n",
      "0.7681818181818181\n",
      "0.5\n",
      "0.8522727272727272\n",
      "0.9045454545454545\n",
      "0.7636363636363637\n",
      "0.7681818181818181\n",
      "0.9045454545454545\n",
      "0.7681818181818181\n",
      "0.9477272727272728\n",
      "0.9022727272727273\n",
      "0.9159090909090909\n",
      "0.85\n",
      "0.7636363636363637\n",
      "0.9386363636363637\n",
      "0.7636363636363637\n",
      "0.8136363636363637\n",
      "0.9159090909090909\n",
      "0.7545454545454544\n",
      "0.8886363636363636\n",
      "0.8363636363636363\n",
      "0.9159090909090909\n",
      "0.9136363636363637\n",
      "0.925\n",
      "0.925\n",
      "0.9068181818181817\n",
      "0.7681818181818181\n",
      "0.9363636363636363\n",
      "0.8545454545454545\n",
      "0.9363636363636363\n",
      "0.9022727272727273\n",
      "0.7545454545454544\n",
      "0.7545454545454544\n",
      "0.7681818181818181\n",
      "0.7545454545454544\n",
      "0.7681818181818181\n",
      "0.8704545454545454\n",
      "0.9159090909090909\n",
      "0.9181818181818181\n",
      "0.7681818181818181\n",
      "0.9272727272727272\n",
      "0.9022727272727273\n",
      "0.9386363636363637\n",
      "0.8568181818181818\n",
      "0.8863636363636365\n",
      "0.7636363636363637\n",
      "0.9272727272727272\n",
      "0.9363636363636363\n",
      "0.925\n",
      "0.9159090909090909\n",
      "0.825\n",
      "0.8840909090909091\n",
      "0.9272727272727272\n",
      "0.9590909090909091\n",
      "0.7636363636363637\n",
      "0.8863636363636365\n",
      "0.9045454545454545\n",
      "0.9159090909090909\n",
      "0.7545454545454544\n",
      "0.925\n",
      "0.8272727272727273\n",
      "0.9340909090909091\n",
      "0.7931818181818182\n",
      "0.9022727272727273\n",
      "0.8386363636363636\n",
      "0.9159090909090909\n",
      "0.7818181818181819\n",
      "0.7909090909090909\n",
      "0.9068181818181817\n",
      "0.9022727272727273\n",
      "0.8931818181818181\n",
      "0.9159090909090909\n",
      "0.8227272727272729\n",
      "0.9159090909090909\n",
      "0.75\n",
      "0.9022727272727273\n",
      "0.7909090909090909\n",
      "0.9181818181818181\n",
      "0.8863636363636365\n",
      "0.9136363636363637\n",
      "0.9159090909090909\n",
      "0.8363636363636363\n",
      "0.8772727272727272\n",
      "0.7772727272727272\n",
      "0.7545454545454544\n",
      "0.9022727272727273\n",
      "0.9045454545454545\n",
      "0.7704545454545455\n",
      "0.8772727272727272\n",
      "0.7659090909090909\n",
      "0.7659090909090909\n",
      "0.9272727272727272\n",
      "0.925\n",
      "0.8909090909090909\n",
      "0.7909090909090909\n",
      "0.925\n",
      "0.7545454545454544\n",
      "0.859090909090909\n",
      "0.7545454545454544\n",
      "0.9045454545454545\n",
      "0.9272727272727272\n",
      "0.9068181818181817\n",
      "0.7681818181818181\n",
      "0.7636363636363637\n",
      "0.7704545454545455\n",
      "0.8659090909090909\n",
      "0.7681818181818181\n",
      "0.6840909090909091\n",
      "0.8909090909090909\n",
      "0.9045454545454545\n",
      "0.9272727272727272\n",
      "0.9045454545454545\n",
      "0.759090909090909\n",
      "0.9022727272727273\n",
      "0.9159090909090909\n",
      "0.7931818181818182\n",
      "0.9181818181818181\n",
      "0.9045454545454545\n",
      "0.9181818181818181\n",
      "0.9272727272727272\n",
      "0.7545454545454544\n",
      "0.9068181818181817\n",
      "0.9363636363636363\n",
      "0.8340909090909091\n",
      "0.925\n",
      "0.9159090909090909\n",
      "0.9159090909090909\n",
      "0.9363636363636363\n",
      "0.9272727272727272\n",
      "0.8795454545454545\n",
      "0.8795454545454545\n",
      "0.8954545454545454\n",
      "0.9022727272727273\n",
      "0.8977272727272727\n",
      "0.9181818181818181\n",
      "0.8681818181818182\n",
      "0.7681818181818181\n",
      "0.9045454545454545\n",
      "0.7886363636363636\n",
      "0.8568181818181818\n",
      "0.825\n",
      "0.8977272727272727\n",
      "0.859090909090909\n",
      "0.8568181818181818\n",
      "0.740909090909091\n",
      "0.9068181818181817\n",
      "0.9363636363636363\n",
      "0.8954545454545454\n",
      "0.925\n",
      "0.9159090909090909\n",
      "0.7659090909090909\n",
      "0.9181818181818181\n",
      "0.8818181818181816\n",
      "0.9363636363636363\n",
      "0.75\n",
      "0.9045454545454545\n",
      "0.9272727272727272\n",
      "0.9068181818181817\n",
      "0.7636363636363637\n",
      "0.9181818181818181\n",
      "0.9272727272727272\n",
      "0.8045454545454546\n",
      "0.7659090909090909\n",
      "0.85\n",
      "0.9068181818181817\n",
      "0.8045454545454546\n",
      "0.9477272727272728\n",
      "0.9181818181818181\n",
      "0.7636363636363637\n",
      "0.7636363636363637\n",
      "0.7681818181818181\n",
      "0.9181818181818181\n",
      "0.9159090909090909\n",
      "0.7636363636363637\n",
      "0.9227272727272727\n",
      "0.7636363636363637\n",
      "0.9159090909090909\n",
      "0.9272727272727272\n",
      "0.9068181818181817\n",
      "0.9181818181818181\n",
      "0.9136363636363637\n",
      "0.9272727272727272\n",
      "0.7909090909090909\n",
      "0.925\n",
      "Number of finished trials: 999\n",
      "Best trial: {'l2_regularization': 0.0034244177726349837, 'early_stopping': False, 'learning_rate': 0.07915435645346343, 'max_depth': 8, 'max_bins': 255, 'min_samples_leaf': 8, 'max_leaf_nodes': 53}\n",
      "Wall time: 32min 59s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "study = optuna.create_study(direction='maximize')\n",
    "optuna.logging.set_verbosity(optuna.logging.ERROR)\n",
    "study.optimize(objective, n_trials=999)\n",
    "print('Number of finished trials:', len(study.trials))\n",
    "print('Best trial:', study.best_trial.params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'l2_regularization': 0.0034244177726349837,\n",
       " 'early_stopping': False,\n",
       " 'learning_rate': 0.07915435645346343,\n",
       " 'max_depth': 8,\n",
       " 'max_bins': 255,\n",
       " 'min_samples_leaf': 8,\n",
       " 'max_leaf_nodes': 53}"
      ]
     },
     "execution_count": 245,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "study.best_trial.params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9590909090909091\n"
     ]
    }
   ],
   "source": [
    "print(study.best_trial.value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8393939393939394\n",
      "Precision_score : 0.8966789667896679\n",
      "recall_score : 0.9067164179104478\n",
      "f1_score: 0.901669758812616\n",
      "ROC AUC score: 0.7275517573423207\n",
      "ROC AUC PROBA: 0.7173206547905633\n",
      "[[ 34  28]\n",
      " [ 25 243]]\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "hgb_1 = HistGradientBoostingClassifier(**study.best_trial.params)\n",
    "hgb_1.fit(X_train,y_train.values.ravel())\n",
    "y_pred = hgb_1.predict(X_test)\n",
    "y_pred_proba = hgb_1.predict_proba(X_test)[:,1]\n",
    "eval(y_test, y_pred, y_pred_proba)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DSClass",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
